<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://kubevirt.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://kubevirt.io//" rel="alternate" type="text/html" /><updated>2022-05-02T15:06:52+00:00</updated><id>https://kubevirt.io//feed.xml</id><title type="html">KubeVirt.io</title><subtitle>Virtual Machine Management on Kubernetes</subtitle><entry><title type="html">KubeVirt v0.52.0</title><link href="https://kubevirt.io//2022/changelog-v0.52.0.html" rel="alternate" type="text/html" title="KubeVirt v0.52.0" /><published>2022-04-08T00:00:00+00:00</published><updated>2022-04-08T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.52.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.52.0.html"><![CDATA[<h2 id="v0520">v0.52.0</h2>

<p>Released on: Fri Apr 8 16:17:56 2022 +0000</p>

<ul>
  <li>[PR #7024][fossedihelm] Add an warning message if the client and server virtctl versions are not aligned</li>
  <li>[PR #7486][rmohr] Move stable.txt location to a more appropriate path</li>
  <li>[PR #7372][saschagrunert] Fixed <code class="language-plaintext highlighter-rouge">KubeVirtComponentExceedsRequestedMemory</code> alert complaining about many-to-many matching not allowed.</li>
  <li>[PR #7426][iholder-redhat] Add warning for manually determining core-component replica count in Kubevirt CR</li>
  <li>[PR #7424][maiqueb] Provide interface binding types descriptions, which will be featured in the KubeVirt API.</li>
  <li>[PR #7422][orelmisan] Fixed setting custom guest pciAddress and bootOrder parameter(s) to a list of SR-IOV NICs.</li>
  <li>[PR #7421][rmohr] Fix knowhosts file corruption for virtctl ssh</li>
  <li>[PR #6854][rmohr] Make virtctl ssh work with ssh-rsa+ preauthentication</li>
  <li>[PR #7267][iholder-redhat] Applied migration configurations can now be found in VMI‚Äôs status</li>
  <li>[PR #7321][iholder-redhat] [Migration Policies]: precedence to VMI labels over Namespace labels</li>
  <li>[PR #7326][oshoval] The Ginkgo dependency has been upgraded to v2.1.3 (major version upgrade)</li>
  <li>[PR #7361][SeanKnight] Fixed a bug that prevents virtctl from working with clusters accessed via Rancher authentication proxy, or any other cluster where the server URL contains a path component. (#3760)</li>
  <li>[PR #7255][tyleraharrison] Users are now able to specify <code class="language-plaintext highlighter-rouge">--address [ip_address]</code> when using <code class="language-plaintext highlighter-rouge">virtctl vnc</code> rather than only using 127.0.0.1</li>
  <li>[PR #7275][enp0s3] Add observedGeneration to virt-operator to have a race-free way to detect KubeVirt config rollouts</li>
  <li>[PR #7233][xpivarc] Bug fix: Successfully aborted migrations should be reported now</li>
  <li>[PR #7158][AlonaKaplan] Add masquerade VMs support to single stack IPv6.</li>
  <li>[PR #7227][rmohr] Remove VMI informer from virt-api to improve scaling characteristics of virt-api</li>
  <li>[PR #7288][raspbeep] Users now don‚Äôt need to specify container for <code class="language-plaintext highlighter-rouge">kubectl logs &lt;vmi-pod&gt;</code> and <code class="language-plaintext highlighter-rouge">kubectl exec &lt;vmi-pod&gt;</code>.</li>
  <li>[PR #6709][xpivarc] Workloads will be migrated to nonroot implementation if NonRoot feature gate is set. (Except VirtioFS)</li>
  <li>[PR #7241][lyarwood] Fixed a bug that prevents only a unattend.xml configmap or secret being provided as contents for a sysprep disk. (#7240, @lyarwood)</li>
</ul>]]></content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.52.0 changes]]></summary></entry><entry><title type="html">KubeVirt v0.51.0</title><link href="https://kubevirt.io//2022/changelog-v0.51.0.html" rel="alternate" type="text/html" title="KubeVirt v0.51.0" /><published>2022-03-08T00:00:00+00:00</published><updated>2022-03-08T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.51.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.51.0.html"><![CDATA[<h2 id="v0510">v0.51.0</h2>

<p>Released on: Tue Mar 8 21:06:59 2022 +0000</p>

<ul>
  <li>[PR #7102][machadovilaca] Add Virtual Machine name label to virt-launcher pod</li>
  <li>[PR #7139][davidvossel] Fixes inconsistent VirtualMachinePool VM/VMI updates by using controller revisions</li>
  <li>[PR #6754][jean-edouard] New and resized disks are now always 1MiB-aligned</li>
  <li>[PR #7086][acardace] Add ‚ÄòEvictionStrategy‚Äô as a cluster-wide setting in the KubeVirt CR</li>
  <li>[PR #7232][rmohr] Properly format the PDB scale event during migrations</li>
  <li>[PR #7223][Barakmor1] Add a name label to virt-operator pods</li>
  <li>[PR #7221][davidvossel] RunStrategy: Once - allows declaring a VM should run once to a finalized state</li>
  <li>[PR #7091][EdDev] SR-IOV interfaces are now reported in the VMI status even without an active guest-agent.</li>
  <li>[PR #7169][rmohr] Improve device plugin de-registration in virt-handler and some test stabilizations</li>
  <li>[PR #6604][alicefr] Add shareable option to identify if the disk is shared with other VMs</li>
  <li>[PR #7144][davidvossel] Garbage collect finalized migration objects only leaving the most recent 5 objects</li>
  <li>[PR #6110][xpivarc] [Nonroot] SRIOV is now available.</li>
</ul>]]></content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.51.0 changes]]></summary></entry><entry><title type="html">KubeVirt v0.50.0</title><link href="https://kubevirt.io//2022/changelog-v0.50.0.html" rel="alternate" type="text/html" title="KubeVirt v0.50.0" /><published>2022-02-09T00:00:00+00:00</published><updated>2022-02-09T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.50.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.50.0.html"><![CDATA[<h2 id="v0500">v0.50.0</h2>

<p>Released on: Wed Feb 9 18:01:08 2022 +0000</p>

<ul>
  <li>[PR #7056][fossedihelm] Update k8s dependencies to 0.23.1</li>
  <li>[PR #7135][davidvossel] Switch from reflects.DeepEquals to equality.Semantic.DeepEquals() across the entire project</li>
  <li>[PR #7052][sradco] Updated recording rule ‚Äúkubevirt_vm_container_free_memory_bytes‚Äù</li>
  <li>[PR #7000][iholder-redhat] Adds a possibility to override default libvirt log filters though VMI annotations</li>
  <li>[PR #7064][davidvossel] Fixes issue associated with blocked uninstalls when VMIs exist during removal</li>
  <li>[PR #7097][iholder-redhat] [Bug fix] VMI with kernel boot stuck on ‚ÄúTerminating‚Äù status if more disks are defined</li>
  <li>[PR #6700][VirrageS] Simplify replacing <code class="language-plaintext highlighter-rouge">time.Ticker</code> in agent poller and fix default values for <code class="language-plaintext highlighter-rouge">qemu-*-interval</code> flags</li>
  <li>[PR #6581][ormergi] SRIOV network interfaces are now hot-plugged when disconnected manually or due to aborted migrations.</li>
  <li>[PR #6924][EdDev] Support for legacy GPU definition is removed. Please see https://kubevirt.io/user-guide/virtual_machines/host-devices on how to define host-devices.</li>
  <li>[PR #6735][uril] The command <code class="language-plaintext highlighter-rouge">migrate_cancel</code> was added to virtctl. It cancels an active VM migration.</li>
  <li>[PR #6883][rthallisey] Add instance-type to cloud-init metadata</li>
  <li>[PR #6999][maya-r] When expanding disk images, take the minimum between the request and the capacity - avoid using the full underlying file system on storage like NFS, local.</li>
  <li>[PR #6946][vladikr] Numa information of an assigned device will be presented in the devices metadata</li>
  <li>[PR #6042][iholder-redhat] Fully support cgroups v2, include a new cohesive package and perform major refactoring.</li>
  <li>[PR #6968][vladikr] Added Writeback disk cache support</li>
  <li>[PR #6995][sradco] Alert OrphanedVirtualMachineImages name was changed to OrphanedVirtualMachineInstances.</li>
  <li>[PR #6923][rhrazdil] Fix issue with ssh being unreachable on VMIs with Istio proxy</li>
  <li>[PR #6821][jean-edouard] Migrating VMIs that contain dedicated CPUs will now have properly dedicated CPUs on target</li>
  <li>[PR #6793][oshoval] Add infoSource field to vmi.status.interfaces.</li>
</ul>]]></content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.50.0 changes]]></summary></entry><entry><title type="html">Dedicated migration network in KubeVirt</title><link href="https://kubevirt.io//2022/Dedicated-migration-network.html" rel="alternate" type="text/html" title="Dedicated migration network in KubeVirt" /><published>2022-01-25T00:00:00+00:00</published><updated>2022-01-25T00:00:00+00:00</updated><id>https://kubevirt.io//2022/Dedicated-migration-network</id><content type="html" xml:base="https://kubevirt.io//2022/Dedicated-migration-network.html"><![CDATA[<p>Since version 0.49, KubeVirt supports live migrating VMIs over a separate network than the one Kubernetes is running on.</p>

<p>Running migrations over a dedicated network is a great way to increase migration bandwidth and reliability.</p>

<p>This article gives an overview of the feature as well as a concrete example. For more technical information, refer to the <a href="https://kubevirt.io/user-guide/operations/live_migration/#using-a-different-network-for-migrations">KubeVirt documentation</a>.</p>

<h2 id="hardware-configuration">Hardware configuration</h2>

<p>The simplest way to use the feature is to find an unused NIC on every worker node, and to connect them all to the same switch.</p>

<p>All NICs must have the same name. If they don‚Äôt, they should be permanently renamed.
The process for renaming NICs varies depending on your operating system, refer to its documentation if you need help.</p>

<p>Adding servers to the network for services like DHCP or DNS is an option but it is not required.
If a DHCP is running, it is best if it doesn‚Äôt provide routes to other networks / the internet, to keep the migration network isolated.</p>

<h2 id="cluster-configuration">Cluster configuration</h2>

<p>The interface between the physical network and KubeVirt is a NetworkAttachmentDefinition (NAD), created in the namespace where KubeVirt is installed.</p>

<p>The implementation of the NAD is up to the admin, as long as it provides a link to the secondary network.
The admin must also ensure that the NAD is able to provide cluster-wide IPs, either through a physical DHCP, or with another CNI plugin like <a href="https://github.com/k8snetworkplumbingwg/whereabouts">whereabouts</a></p>

<p>Important: the subnet used here must be completely distinct from the ones used by the main Kubernetes network, to ensure proper routing.</p>

<h2 id="testing">Testing</h2>

<p>If you just want to test the feature, KubeVirtCI supports the creation of multiple nodes, as well as secondary networks.
All you need is to define the right environment variables before starting the cluster.</p>

<p>See the example below for more info (note that text in the ‚Äúvideo‚Äù can actually be selected and copy/pasted).</p>

<h2 id="example">Example</h2>

<p>Here is a quick <a href="https://asciinema.org/a/464272">example</a> of a dual-node KubeVirtCI cluster running a migration over a secondary network.</p>

<p>The description of the clip includes more detailed information about the steps involved.</p>]]></content><author><name>Jed Lejosne</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="live migration" /><category term="dedicated network" /><summary type="html"><![CDATA[KubeVirt now supports using a separate network for live migrations]]></summary></entry><entry><title type="html">KubeVirt Summit is coming back!</title><link href="https://kubevirt.io//2022/KubeVirt-Summit-2022.html" rel="alternate" type="text/html" title="KubeVirt Summit is coming back!" /><published>2022-01-24T00:00:00+00:00</published><updated>2022-01-24T00:00:00+00:00</updated><id>https://kubevirt.io//2022/KubeVirt-Summit-2022</id><content type="html" xml:base="https://kubevirt.io//2022/KubeVirt-Summit-2022.html"><![CDATA[<p>The second online <a href="/summit/">KubeVirt Summit</a> is coming on February 16, 2022!</p>

<h2 id="when">When</h2>

<p>The event will take place online during two half-days:</p>

<ul>
  <li>Dates: February 16 and 17, 2022.</li>
  <li>Time: 14:00 ‚Äì 19:00 UTC (9:00‚Äì14:00 EST, 15:00‚Äì20:00 CET)</li>
</ul>

<h2 id="register">Register</h2>

<p><a href="/summit/">KubeVirt Summit</a> is hosted on Community.CNCF.io. Because of how that platform works, you need to register for each of the two days of the summit independantly:</p>

<ul>
  <li><a href="https://community.cncf.io/events/details/cncf-kubevirt-community-presents-kubevirt-summit-2022-day-1/">Register for Day 1</a></li>
  <li><a href="https://community.cncf.io/events/details/cncf-kubevirt-community-presents-kubevirt-summit-2022-day-2/">Register for Day 2</a></li>
</ul>

<p>You will need to create an account with CNCF.io if you have not before. Attendance is free.</p>

<h2 id="keep-up-to-date">Keep up to date</h2>

<p>Connect with the KubeVirt Community through our <a href="/community">community page</a>.</p>

<p>We are looking forward to meeting you there!</p>]]></content><author><name>Chandler Wilkerson</name></author><category term="news" /><category term="kubevirt" /><category term="event" /><category term="community" /><summary type="html"><![CDATA[Join us for the KubeVirt community's second annual dedicated online event]]></summary></entry><entry><title type="html">KubeVirt v0.49.0</title><link href="https://kubevirt.io//2022/changelog-v0.49.0.html" rel="alternate" type="text/html" title="KubeVirt v0.49.0" /><published>2022-01-11T00:00:00+00:00</published><updated>2022-01-11T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.49.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.49.0.html"><![CDATA[<h2 id="v0490">v0.49.0</h2>

<p>Released on: Tue Jan 11 17:27:09 2022 +0000</p>

<ul>
  <li>[PR #7004][iholder-redhat] Bugfix: Avoid setting block migration for volumes used by read-only disks</li>
  <li>[PR #6959][enp0s3] generate event when target pod enters unschedulable phase</li>
  <li>[PR #6888][assafad] Added common labels into alert definitions</li>
  <li>[PR #6166][vasiliy-ul] Experimental support of AMD SEV</li>
  <li>[PR #6980][vasiliy-ul] Updated the dependencies to include the fix for CVE-2021-43565 (KubeVirt is not affected)</li>
  <li>[PR #6944][iholder-redhat] Remove disabling TLS configuration from Live Migration Policies</li>
  <li>[PR #6800][jean-edouard] CPU pinning doesn‚Äôt require hardware-assisted virtualization anymore</li>
  <li>[PR #6501][ShellyKa13] Use virtctl image-upload to upload archive content</li>
  <li>[PR #6918][iholder-redhat] Bug fix: Unscheduable host-model VMI alert is now properly triggered</li>
  <li>[PR #6796][Barakmor1] ‚Äòkubevirt-operator‚Äô changed to ‚Äòvirt-operator‚Äô on ‚Äòmanaged-by‚Äô label in kubevirt‚Äôs components made by virt-operator</li>
  <li>[PR #6036][jean-edouard] Migrations can now be done over a dedicated multus network</li>
  <li>[PR #6933][erkanerol] Add a new lane for monitoring tests</li>
  <li>[PR #6949][jean-edouard] KubeVirt components should now be successfully removed on CR deletion, even when using only 1 replica for virt-api and virt-controller</li>
  <li>[PR #6954][maiqueb] Update the <code class="language-plaintext highlighter-rouge">virtctl</code> exposed services <code class="language-plaintext highlighter-rouge">IPFamilyPolicyType</code> default to <code class="language-plaintext highlighter-rouge">IPFamilyPolicyPreferDualStack</code></li>
  <li>[PR #6931][fossedihelm] added DryRun to AddVolumeOptions and RemoveVolumeOptions</li>
  <li>[PR #6379][nunnatsa] Fix issue https://bugzilla.redhat.com/show_bug.cgi?id=1945593</li>
  <li>[PR #6399][iholder-redhat] Introduce live migration policies that allow system-admins to have fine-grained control over migration configuration for different sets of VMs.</li>
  <li>[PR #6880][iholder-redhat] Add full Podman support for <code class="language-plaintext highlighter-rouge">make</code> and <code class="language-plaintext highlighter-rouge">make test</code></li>
  <li>[PR #6702][acardace] implement virt-handler canary upgrade and rollback for faster and safer rollouts</li>
  <li>[PR #6717][davidvossel] Introducing the VirtualMachinePools feature for managing stateful VMs at scale</li>
  <li>[PR #6698][rthallisey] Add tracing to the virt-controller work queue</li>
  <li>[PR #6762][fossedihelm] added DryRun mode to virtcl to migrate command</li>
  <li>[PR #6891][rmohr] Fix ‚ÄúMake raw terminal failed: The handle is invalid?‚Äù issue with ‚Äúvirtctl console‚Äù when not executed in a pty</li>
  <li>[PR #6783][rmohr] Skip SSH RSA auth if no RSA key was explicitly provided and not key exists at the default location</li>
</ul>]]></content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.49.0 changes]]></summary></entry><entry><title type="html">KubeVirt v0.48.0</title><link href="https://kubevirt.io//2021/changelog-v0.48.0.html" rel="alternate" type="text/html" title="KubeVirt v0.48.0" /><published>2021-12-06T00:00:00+00:00</published><updated>2021-12-06T00:00:00+00:00</updated><id>https://kubevirt.io//2021/changelog-v0.48.0</id><content type="html" xml:base="https://kubevirt.io//2021/changelog-v0.48.0.html"><![CDATA[<h2 id="v0480">v0.48.0</h2>

<p>Released on: Mon Dec 6 18:26:51 2021 +0000</p>

<ul>
  <li>[PR #6670][futuretea] Added ‚Äòvirtctl soft-reboot‚Äô command to reboot the VMI.</li>
  <li>[PR #6861][orelmisan] virtctl errors are written to stderr instead of stdout</li>
  <li>[PR #6836][enp0s3] Added PHASE and VMI columns for the ‚Äòkubectl get vmim‚Äô CLI output</li>
  <li>[PR #6784][nunnatsa] kubevirt-config configMap is no longer supported for KubeVirt configuration</li>
  <li>[PR #6839][ShellyKa13] fix restore of VM with RunStrategy</li>
  <li>[PR #6533][zcahana] Paused VMIs are now marked as unready even when no readinessProbe is specified</li>
  <li>[PR #6858][rmohr] Fix a nil pointer in virtctl in combination with some external auth plugins</li>
  <li>[PR #6780][fossedihelm] Add PatchOptions to the Patch request of the VirtualMachineInstanceInterface</li>
  <li>[PR #6773][iholder-redhat] alert if migration for VMI with host-model CPU is stuck since no node is suitable</li>
  <li>[PR #6714][rhrazdil] Shorten timeout for Istio proxy detection</li>
  <li>[PR #6725][fossedihelm] added DryRun mode to virtcl for pause and unpause commands</li>
  <li>[PR #6737][davidvossel] Pending migration target pods timeout after 5 minutes when unschedulable</li>
  <li>[PR #6814][fossedihelm] Changed some terminology to be more inclusive</li>
  <li>[PR #6649][Barakmor1] Designate the apps.kubevirt.io/component label for KubeVirt components.</li>
  <li>[PR #6650][victortoso] Introduces support to ich9 or ac97 sound devices</li>
  <li>[PR #6734][Barakmor1] replacing the command that extract libvirtd‚Äôs pid  to avoid this error:</li>
  <li>[PR #6802][rmohr] Maintain a separate api package which synchronizes to kubevirt.io/api for better third party integration with client-gen</li>
  <li>[PR #6730][zhhray] change kubevrit cert secret type from Opaque to kubernetes.io/tls</li>
  <li>[PR #6508][oshoval] Add missing domain to guest search list, in case subdomain is used.</li>
  <li>[PR #6664][vladikr] enable the display and ramfb for vGPUs by default</li>
  <li>[PR #6710][iholder-redhat] virt-launcher fix - stop logging successful shutdown when it isn‚Äôt true</li>
  <li>[PR #6162][vladikr] KVM_HINTS_REALTIME will always be set when dedicatedCpusPlacement is requested</li>
  <li>[PR #6772][zcahana] Bugfix: revert #6565 which prevented upgrades to v0.47.</li>
  <li>[PR #6722][zcahana] Remove obsolete scheduler.alpha.kubernetes.io/critical-pod annotation</li>
  <li>[PR #6723][acardace] remove stale pdbs created by &lt; 0.41.1 virt-controller</li>
  <li>[PR #6721][iholder-redhat] Set default CPU model in VMI spec, even if not defined in KubevirtCR</li>
  <li>[PR #6713][zcahana] Report WaitingForVolumeBinding VM status when PVC/DV-type volumes reference unbound PVCs</li>
  <li>[PR #6681][fossedihelm] Users can use ‚Äìdry-run flag</li>
  <li>[PR #6663][jean-edouard] The number of virt-api and virt-controller replicas is now configurable in the CSV</li>
  <li>[PR #5981][maya-r] Always resize disk.img files to the largest size at boot.</li>
</ul>]]></content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.48.0 changes]]></summary></entry><entry><title type="html">Running real-time workloads with improved performance</title><link href="https://kubevirt.io//2021/Running-Realtime-Workloads.html" rel="alternate" type="text/html" title="Running real-time workloads with improved performance" /><published>2021-10-13T00:00:00+00:00</published><updated>2021-10-13T00:00:00+00:00</updated><id>https://kubevirt.io//2021/Running-Realtime-Workloads</id><content type="html" xml:base="https://kubevirt.io//2021/Running-Realtime-Workloads.html"><![CDATA[<h2 id="motivation">Motivation</h2>

<p>It has been possible in KubeVirt for some time already to run a VM running with a RT kernel, however the performance of such workloads never achieved parity against running on top of a bare metal host virtualized. With the availability of NUMA and CPUManager as features in KubeVirt, we were close to a point where we had almost all the ingredients to deliver the <a href="https://www.libvirt.org/kbase/kvm-realtime.html">recommended</a> tunings in libvirt for achieving the low CPU latency needed for such workloads. We were missing two important settings:</p>
<ul>
  <li>The ability to configure the VCPUs to run with real-time scheduling policy.</li>
  <li>Lock the VMs huge pages in RAM to prevent swapping.</li>
</ul>

<h2 id="setting-up-the-environment">Setting up the Environment</h2>
<p>To achieve the lowest latency possible in a given environment, first it needs to be configured to allow its resources to be consumed efficiently.</p>

<h3 id="the-cluster">The Cluster</h3>
<p>The target node has to be configured to reserve memory for hugepages and the kernel to allow threads to run with real-time scheduling policy. The memory can be reserved as a <a href="https://www.kernel.org/doc/html/latest/admin-guide/mm/hugetlbpage.html">kernel boot parameter</a> or by changing the kernel‚Äôs page count at <a href="https://www.kernel.org/doc/html/latest/admin-guide/mm/hugetlbpage.html">runtime</a>.</p>

<p>The kernel‚Äôs runtime scheduling limit can be adjusted either by installing a real-time kernel in the node (the recommended option), or changing the kernel‚Äôs setting <code class="language-plaintext highlighter-rouge">kernel.sched_rt_runtime_us</code> to equal -1, to allow for unlimited runtime of real-time scheduled threads. This kernel setting defines the time period to be devoted to running real-time threads. KubeVirt will detect if the node has been configured with unlimited runtime and will label the node with <code class="language-plaintext highlighter-rouge">kubevirt.io/realtime</code> to highlight the capacity of running real-time workloads. Later on we‚Äôll come back to this label when we talk about how the workload is scheduled.</p>

<p>It is also recommended tuning the node‚Äôs BIOS settings for optimal real-time performance is also recommended to achieve even lower CPU latencies. Consult with your hardware provider to obtain the information on how to best tune your equipment.</p>

<h3 id="kubevirt">KubeVirt</h3>
<p>The VM will require to be granted fully dedicated CPUs and be able to use huge pages. These requirements can be achieved in KubeVirt by enabling the feature gates of CPUManager and NUMA in the KubeVirt CR. There is no dedicated feature gate to enable the new real-time optimizations.</p>

<h2 id="the-manifest">The Manifest</h2>
<p>With the cluster configured to provide the dedicated resources for the workload, it‚Äôs time to review an example of a VM manifest using the optimizations for low CPU latency. The first focus is to reduce the VM‚Äôs I/O by limiting it‚Äôs devices to only serial console:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec.domain.devices.autoattachSerialConsole</span><span class="pi">:</span> <span class="no">true</span>
<span class="na">spec.domain.devices.autoattachMemBalloon</span><span class="pi">:</span> <span class="no">false</span>
<span class="na">spec.domain.devices.autoattachGraphicsDevice</span><span class="pi">:</span> <span class="no">false</span>
</code></pre></div></div>

<p>The pod needs to have a guaranteed QoS for its memory and CPU resources, to make sure that the CPU manager will dedicate the requested CPUs to the pod.</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec.domain.resources.request.cpu</span><span class="pi">:</span> <span class="m">2</span>
<span class="na">spec.domain.resources.request.memory</span><span class="pi">:</span> <span class="s">1Gi</span>
<span class="na">spec.domain.resources.limits.cpu</span><span class="pi">:</span> <span class="m">2</span>
<span class="na">spec.domain.resources.limits.memory</span><span class="pi">:</span> <span class="s">1Gi</span>
</code></pre></div></div>

<p>Still on the CPU front, we add the settings to instruct the KVM to give a clear visibility of the host‚Äôs features to the guest, request the CPU manager in the node to isolate the assigned CPUs and to make sure that the emulator and IO threads in the VM run in their own dedicated VCPU rather than sharing the computational time with the workload.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec.domain.cpu.model</span><span class="pi">:</span> <span class="s">host-passthrough</span>
<span class="na">spec.domain.cpu.dedicateCpuPlacement</span><span class="pi">:</span> <span class="no">true</span>
<span class="na">spec.domain.cpu.isolateEmulatorThread</span><span class="pi">:</span> <span class="no">true</span>
<span class="na">spec.domain.cpu.ioThreadsPolicy</span><span class="pi">:</span> <span class="s">auto</span>
</code></pre></div></div>

<p>We also request the huge pages size and guaranteed NUMA topology that will pin the CPU and memory resources to a single NUMA node in the host. The Kubernetes scheduler will perform due diligence to schedule the pod in a node with enough free huge pages of the given size.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec.domain.cpu.numa.guestMappingPassthrough</span><span class="pi">:</span> <span class="pi">{}</span>
<span class="na">spec.domain.memory.hugepages.pageSize</span><span class="pi">:</span> <span class="s">1Gi</span>
</code></pre></div></div>

<p>Lastly, we define the new real-time settings to instruct KubeVirt to apply the real-time scheduling policy for the pinned VCPUs and lock the process memory to avoid from being swapped by the host. In this example, we‚Äôll configure the workload to only apply the real-time scheduling policy to VCPU 0.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec.domain.cpu.realtime.mask</span><span class="pi">:</span> <span class="m">0</span>
</code></pre></div></div>

<p>Alternatively, if no <code class="language-plaintext highlighter-rouge">mask</code> value is specified, all requested CPUs will be configured for real-time scheduling.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec.domain.cpu.realtime</span><span class="pi">:</span> <span class="pi">{}</span>
</code></pre></div></div>

<p>The following yaml is a complete manifest including all the settings we just reviewed.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">fedora-realtime</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fedora-realtime</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">poc</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">fedora-realtime</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">autoattachSerialConsole</span><span class="pi">:</span> <span class="no">true</span>
          <span class="na">autoattachMemBalloon</span><span class="pi">:</span> <span class="no">false</span>
          <span class="na">autoattachGraphicsDevice</span><span class="pi">:</span> <span class="no">false</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>      
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1Gi</span>
            <span class="na">cpu</span><span class="pi">:</span> <span class="m">2</span>
          <span class="na">limits</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1Gi</span>
            <span class="na">cpu</span><span class="pi">:</span> <span class="m">2</span>
        <span class="na">cpu</span><span class="pi">:</span>
          <span class="na">model</span><span class="pi">:</span> <span class="s">host-passthrough</span>
          <span class="na">dedicatedCpuPlacement</span><span class="pi">:</span> <span class="no">true</span>
          <span class="na">isolateEmulatorThread</span><span class="pi">:</span> <span class="no">true</span>
          <span class="na">ioThreadsPolicy</span><span class="pi">:</span> <span class="s">auto</span>
          <span class="na">features</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">tsc-deadline</span>
              <span class="na">policy</span><span class="pi">:</span> <span class="s">require</span>
          <span class="na">numa</span><span class="pi">:</span>
            <span class="na">guestMappingPassthrough</span><span class="pi">:</span> <span class="pi">{}</span>
          <span class="na">realtime</span><span class="pi">:</span>
            <span class="na">mask</span><span class="pi">:</span> <span class="s2">"</span><span class="s">0"</span>
        <span class="na">memory</span><span class="pi">:</span>
          <span class="na">hugepages</span><span class="pi">:</span>
            <span class="na">pageSize</span><span class="pi">:</span> <span class="s">1Gi</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-realtime-container-disk:20211008_5a22acb18</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">password: fedora</span>
            <span class="s">chpasswd: { expire: False }</span>
            <span class="s">bootcmd:</span>
              <span class="s">- tuned-adm profile realtime</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
</code></pre></div></div>

<h2 id="the-deployment">The Deployment</h2>
<p>Because the manifest has enabled the real-time setting, when deployed KubeVirt applies the node label selector so that the Kubernetes scheduler will place the deployment in a node that is able to run threads with real-time scheduling policy (node label <code class="language-plaintext highlighter-rouge">kubevirt.io/realtime</code>). But there‚Äôs more, because the manifest also specifies the pod‚Äôs resource need of dedicated CPUs, KubeVirt will also add the node selector of <code class="language-plaintext highlighter-rouge">cpumanager=true</code> to guarantee that the pod is able to use the assigned CPUs alone. And finally, the scheduler also takes care of guaranteeing that the target node has sufficient free huge pages of the specified size (1Gi in our example) to satisfy the memory requested. With all these validations checked, the pod is successfully scheduled.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<p>Being able to run real-time workloads in KubeVirt with lower CPU latency opens new possibilities and expands the use cases where KubeVirt can assist in migrating legacy VMs into the cloud. Real-time workloads are extremely sensitive to the amount of layers between the bare metal and its runtime: the more layers in between, the higher the latency will be. The changes introduced in KubeVirt help reduce such waste and provide lower CPU latencies as the hardware is more efficiently tuned.</p>]]></content><author><name>Jordi Gil</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="real-time" /><category term="NUMA" /><category term="CPUManager" /><summary type="html"><![CDATA[This blog post details the various enhancements made to improve the performance of real-time workloads in KubeVirt]]></summary></entry><entry><title type="html">KubeVirt v0.46.0</title><link href="https://kubevirt.io//2021/changelog-v0.46.0.html" rel="alternate" type="text/html" title="KubeVirt v0.46.0" /><published>2021-10-08T00:00:00+00:00</published><updated>2021-10-08T00:00:00+00:00</updated><id>https://kubevirt.io//2021/changelog-v0.46.0</id><content type="html" xml:base="https://kubevirt.io//2021/changelog-v0.46.0.html"><![CDATA[<h2 id="v0460">v0.46.0</h2>

<p>Released on: Fri Oct 8 21:12:33 2021 +0000</p>

<ul>
  <li>[PR #6425][awels] Hotplug disks are possible when iothreads are enabled.</li>
  <li>[PR #6297][acardace] mutate migration PDBs instead of creating an additional one for the duration of the migration.</li>
  <li>[PR #6464][awels] BugFix: Fixed hotplug race between kubelet and virt-handler when virt-launcher dies unexpectedly.</li>
  <li>[PR #6465][salanki] Fix corrupted DHCP Gateway Option from local DHCP server, leading to rejected IP configuration on Windows VMs.</li>
  <li>[PR #6458][vladikr] Tagged SR-IOV interfaces will now appear in the config drive metadata</li>
  <li>[PR #6446][brybacki] Access mode for virtctl image upload is now optional. This version of virtctl now requires CDI v1.34 or greater</li>
  <li>[PR #6391][zcahana] Cleanup obsolete permissions from virt-operator‚Äôs ClusterRole</li>
  <li>[PR #6419][rthallisey] Fix virt-controller panic caused by lots of deleted VMI events</li>
  <li>[PR #5972][kwiesmueller] Add a <code class="language-plaintext highlighter-rouge">ssh</code> command to <code class="language-plaintext highlighter-rouge">virtctl</code> that can be used to open SSH sessions to VMs/VMIs.</li>
  <li>[PR #6403][jrife] Removed go module pinning to an old version (v0.3.0) of github.com/go-kit/kit</li>
  <li>[PR #6367][brybacki] virtctl imageupload now uses DataVolume.spec.storage</li>
  <li>[PR #6198][iholder-redhat] Fire a Prometheus alert when a lot of REST failures are detected in virt-api</li>
  <li>[PR #6211][davidvossel] cluster-profiler pprof gathering tool and corresponding ‚ÄúClusterProfiler‚Äù feature gate</li>
  <li>[PR #6323][vladikr] switch live migration to use unix sockets</li>
  <li>[PR #6374][vladikr] Fix the default setting of CPU requests on vmipods</li>
  <li>[PR #6283][rthallisey] Record the time it takes to delete a VMI and expose it as a metric</li>
  <li>[PR #6251][rmohr] Better place vcpu threads on host cpus to form more efficient passthrough architectures</li>
  <li>[PR #6377][rmohr] Don‚Äôt fail on failed selinux relabel attempts if selinux is permissive</li>
  <li>[PR #6308][awels] BugFix: hotplug was broken when using it with a hostpath volume that was on a separate device.</li>
  <li>[PR #6186][davidvossel] Add resource and verb labels to rest_client_requests_total metric</li>
</ul>]]></content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.46.0 changes]]></summary></entry><entry><title type="html">Import AWS AMIs as KubeVirt Golden Images</title><link href="https://kubevirt.io//2021/Importing-EC2-to-KubeVirt.html" rel="alternate" type="text/html" title="Import AWS AMIs as KubeVirt Golden Images" /><published>2021-09-21T00:00:00+00:00</published><updated>2021-09-21T00:00:00+00:00</updated><id>https://kubevirt.io//2021/Importing-EC2-to-KubeVirt</id><content type="html" xml:base="https://kubevirt.io//2021/Importing-EC2-to-KubeVirt.html"><![CDATA[<h2 id="breaking-out">Breaking Out</h2>

<p>There comes a point where an operations team has invested so heavily in a Iaas platform that they are effectively locked into that platform. For example, here‚Äôs one scenario outlining how this can happen. An operations team has created automation around building VM images and keeping images up-to-date. In AWS that automation likely involves starting an EC2 instance, injecting some application logic into that instance, sealing the instance‚Äôs boot source as an AMI, and finally copying that AMI around to all the AWS regions the team deploys in.</p>

<p>If the team was interested in evaluating KubeVirt as an alternative Iaas platform to AWS‚Äôs EC2, given the team‚Äôs existing tooling there‚Äôs not a clear path for doing this. It‚Äôs that scenario where the tooling in the <a href="https://github.com/davidvossel/kubevirt-cloud-import">kubevirt-cloud-import</a> project comes into play.</p>

<h2 id="kubevirt-cloud-import">Kubevirt Cloud Import</h2>

<p>The <a href="https://github.com/davidvossel/kubevirt-cloud-import">KubeVirt Cloud Import</a> project explores the practicality of transitioning VMs from various cloud providers into KubeVirt. As of writing this, automation for exporting AMIs from EC2 into KubeVirt works, and it‚Äôs really not all that complicated.</p>

<p>This blog post will explore the fundamentals of how AMIs are exported, and how the KubeVirt Cloud Import project leverages these techniques to build automation pipelines.</p>

<h2 id="nuts-and-bolts-of-importing-amis">Nuts and Bolts of Importing AMIs</h2>

<h3 id="official-aws-ami-export-support">Official AWS AMI Export Support</h3>

<p>AWS supports an <a href="https://docs.aws.amazon.com/vm-import/latest/userguide/vmexport_image.html">api</a> for exporting AMIs as a file to an s3 bucket. This support works quite well, however there‚Äôs a long list of <a href="https://docs.aws.amazon.com/vm-import/latest/userguide/vmexport_image.html#limits-image-export">limitations</a> that impact what AMIs are eligible for export. The most limiting of those items is the one that prevents any image built from an AMI on the marketplace from being eligible for the official export support.</p>

<h3 id="unofficial-aws-export-support">Unofficial AWS export Support</h3>

<p>Regardless of what AWS officially supports or not, there‚Äôs absolutely nothing preventing someone from exporting an AMI‚Äôs contents themselves. The technique just involves creating an EC2 instance, attaching an EBS volume (containing the AMI contents) as a block device, then streaming that block devices contents where ever you want.</p>

<p>Theoretically, the steps roughly look like this.</p>

<ul>
  <li>Convert AMI to a volume by finding the underlying AMI‚Äôs snapshot and converting it to an EBS volume.</li>
  <li>Create an EC2 instance with the EBS volume containing the AMI contents as a secondary data device.</li>
  <li>Within the EC2 guest, copy the EBS device‚Äôs contents as a disk img <code class="language-plaintext highlighter-rouge">dd if=/dev/xvda of=/tmp/disk/disk.img</code></li>
  <li>Then upload the disk image to an object store like s3. <code class="language-plaintext highlighter-rouge">aws s3 cp /tmp/disk/disk.img s3://my-b1-bucket/ upload: ../tmp/disk/disk.img to s3://my-b1-bucket/disk.img</code></li>
</ul>

<h3 id="basics-of-importing-data-into-kubevirt">Basics of Importing Data into KubeVirt</h3>

<p>Once a disk image is in s3, a KubeVirt companion project called the <a href="https://github.com/kubevirt/containerized-data-importer">Containerized Data Importer</a> (or CDI for short) can be used to import the disk from s3 into a PVC within the KubeVirt cluster. This import flow can be expressed as a CDI DataVolume custom resource.</p>

<p>Below is an example yaml for importing s3 contents into a PVC using a DataVolume</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">cdi.kubevirt.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">DataVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">example-import-dv"</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">source</span><span class="pi">:</span>
      <span class="na">s3</span><span class="pi">:</span>
         <span class="na">url</span><span class="pi">:</span> <span class="s2">"</span><span class="s">https://s3.us-west-2.amazonaws.com/my-ami-exports/kubevirt-image-exports/export-ami-0dc4e69702f74df50.vmdk"</span>
         <span class="na">secretRef</span><span class="pi">:</span> <span class="s2">"</span><span class="s">my-s3-credentials"</span>
  <span class="na">pvc</span><span class="pi">:</span>
    <span class="na">accessModes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
    <span class="na">resources</span><span class="pi">:</span>
      <span class="na">requests</span><span class="pi">:</span>
        <span class="na">storage</span><span class="pi">:</span> <span class="s2">"</span><span class="s">6Gi"</span>
</code></pre></div></div>

<p>Once the AMI file content is stored in a PVC, CDI can be used further to clone that AMI‚Äôs PVC on a per VM basis. This effectively recreates the AMI to EC2 relationship that exists in AWS. You can find more information about CDI <a href="https://github.com/kubevirt/containerized-data-importer">here</a></p>

<h2 id="automating-ami-import">Automating AMI import</h2>

<p>Using the technique of exporting an AMI to an s3 bucket and importing the AMI from s3 into a KubeVirt cluster using CDI, the Kubevirt Cloud Import project provides the glue necessary for tying all of these pieces together in the form of the <code class="language-plaintext highlighter-rouge">import-ami</code> cli command and a Tekton task.</p>

<h2 id="automation-using-the-import-ami-cli-command">Automation using the import-ami CLI command</h2>

<p>The <code class="language-plaintext highlighter-rouge">import-ami</code> takes a set of arguments related to the AMI you wish to import into KubeVirt and the name of the PVC you‚Äôd like the AMI to be imported into. Upon execution, import-ami will call all the appropriate AWS and KubeVirt APIs to make this work. The result is a PVC with the AMI contents that is capable of being launched by a KubeVirt VM.</p>

<p>In the example below, A publicly shared <a href="https://alt.fedoraproject.org/cloud/">fedora34 AMI</a> is imported into the KubeVirt cluster as a PVC called fedora34-golden-image</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nb">export </span><span class="nv">S3_BUCKET</span><span class="o">=</span>my-bucket
<span class="nb">export </span><span class="nv">S3_SECRET</span><span class="o">=</span>s3-readonly-cred
<span class="nb">export </span><span class="nv">AWS_REGION</span><span class="o">=</span>us-west-2
<span class="nb">export </span><span class="nv">AMI_ID</span><span class="o">=</span>ami-00a4fdd3db8bb2851
<span class="nb">export </span><span class="nv">PVC_STORAGECLASS</span><span class="o">=</span>rook-ceph-block
<span class="nb">export </span><span class="nv">PVC_NAME</span><span class="o">=</span>fedora34-golden-image

import-ami <span class="nt">--s3-bucket</span> <span class="nv">$S3_BUCKET</span> <span class="nt">--region</span> <span class="nv">$AWS_REGION</span> <span class="nt">--ami-id</span> <span class="nv">$AMI_ID</span> <span class="nt">--pvc-storageclass</span> <span class="nv">$PVC_STORAGECLASS</span> <span class="nt">--s3-secret</span> <span class="nv">$S3_SECRET</span> <span class="nt">--pvc-name</span> <span class="nv">$PVC_NAME</span>

</code></pre></div></div>

<h2 id="automation-using-the-import-ami-tekton-task">Automation using the import-ami Tekton Task</h2>

<p>In addition to the <code class="language-plaintext highlighter-rouge">import-ami</code> cli command, the KubeVirt Cloud Import project also includes a <a href="https://github.com/davidvossel/kubevirt-cloud-import/blob/main/tasks/import-ami/manifests/import-ami.yaml">Tekton task</a> which wraps the cli command and allows integrating AMI import into a Tekton pipeline.</p>

<p>Using a Tekton pipeline, someone can combine the task of importing an AMI into KubeVirt with the task of starting a VM using that AMI. An example pipeline can be found <a href="https://raw.githubusercontent.com/davidvossel/kubevirt-cloud-import/main/examples/create-vm-from-ami-pipeline.yaml">here</a> which outlines how this is accomplished.</p>

<p>Below is a pipeline run that uses the example pipeline to import the publicly shared fedora34 AMI into a PVC, then starts a VM using that imported AMI.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt; pipeline-run.yaml
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: my-vm-creation-pipeline
  namespace: default
spec:
  serviceAccountName: my-kubevirt-service-account
  pipelineRef:
    name: create-vm-pipeline 
  params:
    - name: vmName
      value: vm-fedora34
    - name: s3Bucket
      value: my-kubevirt-exports
    - name: s3ReadCredentialsSecret
      value: my-s3-read-only-credentials
    - name: awsRegion
      value: us-west-2
    - name: amiId 
      value: ami-00a4fdd3db8bb2851
    - name: pvcStorageClass 
      value: rook-ceph-block
    - name: pvcName
      value: fedora34
    - name: pvcNamespace
      value: default
    - name: pvcSize
      value: 6Gi
    - name: pvcAccessMode
      value: ReadWriteOnce
    - name: awsCredentialsSecret
      value: my-aws-credentials
</span><span class="no">EOF

</span>kubectl create <span class="nt">-f</span> pipeline-run.yaml
</code></pre></div></div>

<p>After posting the pipeline run, watch for the pipeline run to complete.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get pipelinerun
selecting docker as container runtime
NAME                      SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME
my-vm-creation-pipeline   True        Succeeded   11m         9m54s
</code></pre></div></div>

<p>Then observe that the resulting VM is online</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get vmi
selecting docker as container runtime
NAME          AGE   PHASE     IP               NODENAME   READY
vm-fedora34   11m   Running   10.244.196.175   node01     True
</code></pre></div></div>

<p>For more detailed and up-to-date information about how to automate AMI import using Tekton, view the KubeVirt Cloud Import <a href="https://github.com/davidvossel/kubevirt-cloud-import/blob/main/README.md">README.md</a></p>

<h2 id="key-takeaways">Key Takeaways</h2>

<p>The portability of workloads across different environments is becoming increasingly important and operations teams need to be vigilant about avoiding vendor lock in. For containers, Kubernetes is an attractive option because it provides a consistent API layer that can run across multiple cloud platforms. KubeVirt can provide that same level of consistency for VMs. As a community we need to invest further into automation tools that allow people to make the transition to KubeVirt.</p>]]></content><author><name>David Vossel</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="AWS" /><category term="EC2" /><category term="AMI" /><summary type="html"><![CDATA[This blog post outlines the fundamentals for how to import VMs from AWS into KubeVirt]]></summary></entry></feed>